{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Assignment 5 (Week 4)\n",
        "\n",
        "## Video Processing and Contour Detection Using OpenCV\n",
        "\n",
        "In this assignment, we'll implement three applications using OpenCV:\n",
        "1. Video I/O Application\n",
        "2. Motion Detection Application with Foreground Mass and Erosion\n",
        "3. Contour Detection Application\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Check and install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib.util\n",
        "\n",
        "def check_and_install_package(package_name, import_name=None):\n",
        "    \"\"\"Check if a package is installed, and install it if it's not.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package_name\n",
        "    \n",
        "    if importlib.util.find_spec(import_name) is None:\n",
        "        print(f\"{package_name} not found. Installing...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"{package_name} installed successfully!\")\n",
        "    else:\n",
        "        print(f\"{package_name} is already installed.\")\n",
        "\n",
        "# List of required packages\n",
        "required_packages = [\n",
        "    (\"opencv-python\", \"cv2\"),\n",
        "    (\"numpy\", \"numpy\"),\n",
        "    (\"matplotlib\", \"matplotlib\"),\n",
        "    (\"ipython\", \"IPython\")\n",
        "]\n",
        "\n",
        "# Check and install each package\n",
        "for package, import_name in required_packages:\n",
        "    check_and_install_package(package, import_name)\n",
        "\n",
        "print(\"All required packages are installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenCV version: 4.11.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Verify OpenCV installation\n",
        "print(f\"OpenCV version: {cv2.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Part 1: Video I/O Application\n",
        "\n",
        "In this part, we'll:\n",
        "- Read a video file\n",
        "- Apply transformations to each frame\n",
        "- Write the processed frames to a new video file\n",
        "\n",
        "We'll implement a split-screen effect showing the original video on the left and a blurred version on the right.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Define the input video path\n",
        "video_path = \"../Visuals/Main.mp4\"\n",
        "\n",
        "# Create a VideoCapture object\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if the video was opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "else:\n",
        "    # Get video properties\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    print(f\"Video Properties:\")\n",
        "    print(f\"Width: {frame_width}, Height: {frame_height}\")\n",
        "    print(f\"FPS: {fps}\")\n",
        "    print(f\"Total Frames: {total_frames}\")\n",
        "    print(f\"Duration: {total_frames/fps:.2f} seconds\")\n",
        "    \n",
        "    # Define the codec and create a VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    output_path = \"output_part1.avi\"\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "    \n",
        "    # Process the video\n",
        "    frame_count = 0\n",
        "    start_time = time.time()\n",
        "    \n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        if not ret:\n",
        "            break\n",
        "        \n",
        "        # Apply transformations:\n",
        "        # 1. Add a timestamp\n",
        "        timestamp = f\"Frame: {frame_count} | Time: {frame_count/fps:.2f}s\"\n",
        "        cv2.putText(frame, timestamp, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    0.8, (0, 255, 0), 2)\n",
        "        \n",
        "        # 2. Add a border\n",
        "        frame = cv2.rectangle(frame, (0, 0), (frame_width-1, frame_height-1), \n",
        "                             (0, 0, 255), 2)\n",
        "        \n",
        "        # 3. Apply a slight blur for demonstration\n",
        "        blurred = cv2.GaussianBlur(frame, (5, 5), 0)\n",
        "        \n",
        "        # 4. Create a split-screen effect: original on left, blurred on right\n",
        "        combined = frame.copy()\n",
        "        mid_point = frame_width // 2\n",
        "        combined[:, mid_point:] = blurred[:, mid_point:]\n",
        "        \n",
        "        # Add a dividing line\n",
        "        cv2.line(combined, (mid_point, 0), (mid_point, frame_height), (0, 255, 255), 2)\n",
        "        \n",
        "        # Add labels\n",
        "        cv2.putText(combined, \"Original\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    0.7, (255, 255, 255), 2)\n",
        "        cv2.putText(combined, \"Blurred\", (mid_point + 10, 60), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    0.7, (255, 255, 255), 2)\n",
        "        \n",
        "        # Write the frame to the output video\n",
        "        out.write(combined)\n",
        "        \n",
        "        # Display progress\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Processed {frame_count}/{total_frames} frames ({frame_count/total_frames*100:.1f}%)\")\n",
        "        \n",
        "        frame_count += 1\n",
        "    \n",
        "    # Calculate processing time\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Processing completed in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Average processing speed: {frame_count/elapsed_time:.2f} frames per second\")\n",
        "    \n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    \n",
        "    print(f\"Output video saved to: {output_path}\")\n",
        "    \n",
        "    # Display a sample frame (we'll use the last processed frame)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(cv2.cvtColor(combined, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Sample Processed Frame\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Part 2: Motion Detection Application with Foreground Mass and Erosion\n",
        "\n",
        "In this part, we'll:\n",
        "- Create a background model from initial frames\n",
        "- Compute the foreground mass (difference between current frame and background)\n",
        "- Apply erosion to remove noise\n",
        "- Annotate frames with bounding boxes around motion areas\n",
        "\n",
        "### Explanation of Concepts:\n",
        "\n",
        "**Foreground Mass**: This is the difference between the current frame and the background model. It helps identify moving objects in the scene.\n",
        "\n",
        "**Erosion**: A morphological operation that removes small, isolated pixels (noise) from the foreground mask. It works by shrinking the boundaries of foreground objects, effectively eliminating small noise pixels while preserving larger motion areas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Define the input video path\n",
        "video_path = \"../Visuals/Main.mp4\"\n",
        "\n",
        "# Create a VideoCapture object\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if the video was opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "else:\n",
        "    # Get video properties\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    \n",
        "    # Define the codec and create a VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    output_path = \"output_part2.avi\"\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height*2))\n",
        "    \n",
        "    # Initialize variables for background subtraction\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Could not read first frame.\")\n",
        "    else:\n",
        "        # Convert to grayscale for background modeling\n",
        "        prev_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        \n",
        "        # Create a background model by averaging the first few frames\n",
        "        num_bg_frames = 10\n",
        "        bg_model = np.zeros_like(prev_gray, dtype=np.float32)\n",
        "        \n",
        "        for i in range(num_bg_frames):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            bg_model += gray / num_bg_frames\n",
        "        \n",
        "        # Convert background model to uint8\n",
        "        bg_model = bg_model.astype(np.uint8)\n",
        "        \n",
        "        # Reset video to beginning\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "        \n",
        "        # Process the video\n",
        "        frame_count = 0\n",
        "        \n",
        "        # Create a kernel for erosion\n",
        "        kernel = np.ones((5, 5), np.uint8)\n",
        "        \n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            \n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            # Convert current frame to grayscale\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            \n",
        "            # Compute foreground mask (difference between current frame and background)\n",
        "            foreground_mask = cv2.absdiff(gray, bg_model)\n",
        "            \n",
        "            # Apply threshold to get binary mask\n",
        "            _, thresh = cv2.threshold(foreground_mask, 30, 255, cv2.THRESH_BINARY)\n",
        "            \n",
        "            # Apply erosion to remove noise\n",
        "            eroded_mask = cv2.erode(thresh, kernel, iterations=1)\n",
        "            \n",
        "            # Find contours in the eroded mask\n",
        "            contours, _ = cv2.findContours(eroded_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            \n",
        "            # Create a copy of the original frame for drawing\n",
        "            result_frame = frame.copy()\n",
        "            \n",
        "            # Draw bounding boxes around detected motion areas\n",
        "            for contour in contours:\n",
        "                # Filter out small contours\n",
        "                if cv2.contourArea(contour) > 100:\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    cv2.rectangle(result_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            \n",
        "            # Add text to explain what's happening\n",
        "            cv2.putText(result_frame, \"Motion Detection\", (10, 30), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "            \n",
        "            # Create visualization of masks\n",
        "            # Convert masks to 3-channel for display\n",
        "            foreground_mask_color = cv2.cvtColor(foreground_mask, cv2.COLOR_GRAY2BGR)\n",
        "            eroded_mask_color = cv2.cvtColor(eroded_mask, cv2.COLOR_GRAY2BGR)\n",
        "            \n",
        "            # Add labels to the masks\n",
        "            cv2.putText(foreground_mask_color, \"Foreground Mask\", (10, 30), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "            cv2.putText(eroded_mask_color, \"Eroded Mask\", (10, 30), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "            \n",
        "            # Create a split view: top half shows original with bounding boxes, \n",
        "            # bottom half shows masks side by side\n",
        "            masks_combined = np.hstack((foreground_mask_color, eroded_mask_color))\n",
        "            \n",
        "            # Resize if necessary to match frame width\n",
        "            if masks_combined.shape[1] != frame_width:\n",
        "                masks_combined = cv2.resize(masks_combined, (frame_width, frame_height))\n",
        "            \n",
        "            # Stack the result frame and masks vertically\n",
        "            output_frame = np.vstack((result_frame, masks_combined))\n",
        "            \n",
        "            # Write the frame to the output video\n",
        "            out.write(output_frame)\n",
        "            \n",
        "            # Update frame count\n",
        "            frame_count += 1\n",
        "            \n",
        "            # Display progress\n",
        "            if frame_count % 30 == 0:\n",
        "                print(f\"Processed {frame_count} frames\")\n",
        "        \n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        \n",
        "        print(f\"Output video saved to: {output_path}\")\n",
        "        \n",
        "        # Display the last output frame\n",
        "        plt.figure(figsize=(12, 16))\n",
        "        plt.imshow(cv2.cvtColor(output_frame, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Motion Detection with Foreground Mass and Erosion\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Part 3: Contour Detection Application\n",
        "\n",
        "In this part, we'll:\n",
        "- Extract a frame from the video to use as our image\n",
        "- Convert the image to grayscale and threshold it\n",
        "- Detect contours using cv2.findContours\n",
        "- Draw the contours on the original image\n",
        "- (Bonus) Extend the solution to detect and draw contours in a video\n",
        "\n",
        "### Explanation of Contours:\n",
        "\n",
        "Contours are curves joining all continuous points along a boundary that have the same color or intensity. They are useful for shape analysis, object detection, and recognition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Part 3A: Contour detection on a single frame\n",
        "\n",
        "# Extract a frame from the video to use as our image\n",
        "video_path = \"../Visuals/Main.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Skip to a frame with interesting content\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, 50)\n",
        "ret, frame = cap.read()\n",
        "\n",
        "if not ret:\n",
        "    print(\"Error: Could not read frame from video.\")\n",
        "else:\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    \n",
        "    # Apply binary threshold\n",
        "    _, thresh = cv2.threshold(blurred, 100, 255, cv2.THRESH_BINARY)\n",
        "    \n",
        "    # Find contours\n",
        "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    \n",
        "    print(f\"Found {len(contours)} contours\")\n",
        "    \n",
        "    # Create a copy of the original frame for drawing\n",
        "    contour_image = frame.copy()\n",
        "    \n",
        "    # Draw all contours\n",
        "    cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)\n",
        "    \n",
        "    # Add text explaining contours\n",
        "    cv2.putText(contour_image, f\"Contours: {len(contours)}\", (10, 30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "    \n",
        "    # Create a side-by-side comparison\n",
        "    comparison = np.hstack((frame, contour_image))\n",
        "    \n",
        "    # Display the comparison\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.imshow(cv2.cvtColor(comparison, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Original Image vs. Contour Detection\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "    # Explanation of the contour detection process\n",
        "    print(\"Contour Detection Process:\")\n",
        "    print(\"1. Convert the image to grayscale to simplify processing\")\n",
        "    print(\"2. Apply Gaussian blur to reduce noise and improve contour detection\")\n",
        "    print(\"3. Apply binary threshold to create a black and white image\")\n",
        "    print(\"4. Find contours using cv2.findContours\")\n",
        "    print(\"5. Draw the contours on the original image\")\n",
        "    \n",
        "    # Explanation of the contour hierarchy\n",
        "    print(\"\\nContour Hierarchy:\")\n",
        "    print(\"The hierarchy describes the relationship between contours.\")\n",
        "    print(\"For each contour, it stores information about:\")\n",
        "    print(\"- Next contour at the same level\")\n",
        "    print(\"- Previous contour at the same level\")\n",
        "    print(\"- First child contour\")\n",
        "    print(\"- Parent contour\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Part 3B: Bonus - Contour detection on video\n",
        "\n",
        "# Reset video to beginning\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "# Get video properties\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Define the codec and create a VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "output_path = \"output_part3.avi\"\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width*2, frame_height))\n",
        "\n",
        "# Process the video\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    \n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Apply Gaussian blur\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    \n",
        "    # Apply threshold\n",
        "    _, thresh = cv2.threshold(blurred, 100, 255, cv2.THRESH_BINARY)\n",
        "    \n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    \n",
        "    # Create a copy for drawing\n",
        "    contour_frame = frame.copy()\n",
        "    \n",
        "    # Draw contours\n",
        "    cv2.drawContours(contour_frame, contours, -1, (0, 255, 0), 2)\n",
        "    \n",
        "    # Add text\n",
        "    cv2.putText(contour_frame, f\"Contours: {len(contours)}\", (10, 30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "    \n",
        "    # Create side-by-side view\n",
        "    output_frame = np.hstack((frame, contour_frame))\n",
        "    \n",
        "    # Write to video\n",
        "    out.write(output_frame)\n",
        "    \n",
        "    # Update frame count\n",
        "    frame_count += 1\n",
        "    \n",
        "    # Display progress\n",
        "    if frame_count % 30 == 0:\n",
        "        print(f\"Processed {frame_count} frames\")\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"Output video saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "In this assignment, we implemented three OpenCV applications:\n",
        "\n",
        "1. **Video I/O Application**: We read a video file, applied transformations (timestamp, border, split-screen with blur), and wrote the processed frames to a new video file.\n",
        "\n",
        "2. **Motion Detection Application**: We created a background model, computed the foreground mass, applied erosion to remove noise, and annotated frames with bounding boxes around motion areas.\n",
        "\n",
        "3. **Contour Detection Application**: We detected and drew contours on both a single frame and throughout a video.\n",
        "\n",
        "These applications demonstrate the power and versatility of OpenCV for video processing and computer vision tasks.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
