{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 (Week 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Video Processing and Contour Detection Using OpenCV\n",
    "\n",
    "reference:  https://colab.research.google.com/drive/1_FGt6GyViY1BAK2dIpf5DFjpH79zBV5E?usp=sharingLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "In this assignment, you will gain hands-on experience with OpenCV by building three applications in a Google Colab notebook:\n",
    "\n",
    "1. **Video I/O Application:** Read from and write to a video stream. You can modify or extend the code we used in class.\n",
    "2. **Motion Detection Application:** Develop a motion detection system that explains the concepts of foreground mass (the difference between the background model and the current frame) and erosion (a morphological operation used to remove noise). You should demonstrate erosion on a new video clip.\n",
    "3. **Contour Detection Application:** Write code that identifies and draws contours on an image. For extra credit, extend your solution to detect and draw contours in a video (with a clip that is less than one minute long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Video I/O Application\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- **Input:** Use any video file of your choice (or create one, e.g., using your webcam or a synthetic video clip that is less than 1 minute long).\n",
    "- **Processing:** Implement a simple transformation on each frame (this can be as basic as converting to grayscale, overlaying text, or drawing shapes).\n",
    "- **Output:** Write the processed frames to a new video file.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Reading the Video:**\n",
    "    - Use cv2.VideoCapture to open the video file.\n",
    "    - Loop through each frame and perform your processing.\n",
    "2. **Processing the Frames:**\n",
    "    - Apply your chosen transformation (for example, add a timestamp overlay or convert to grayscale).\n",
    "3. **Writing the Video:**\n",
    "    - Use cv2.VideoWriter to create an output video file.\n",
    "    - Write the processed frames to this file.\n",
    "\n",
    "Hint: Ensure your output video settings (frame size, codec, and frame rate) match the input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'mp42mp41', 'creation_time': '2025-03-04T12:03:28.000000Z'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': 'eng', 'default': True, 'size': [640, 360], 'bitrate': 1200, 'fps': 29.97002997002997, 'codec_name': 'h264', 'profile': '(Main)', 'metadata': {'Metadata': '', 'creation_time': '2025-03-04T12:03:28.000000Z', 'handler_name': '?Mainconcept Video Media Handler', 'vendor_id': '[0][0][0][0]', 'encoder': 'AVC Coding'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': 'eng', 'default': True, 'fps': 48000, 'bitrate': 157, 'metadata': {'Metadata': '', 'creation_time': '2025-03-04T12:03:28.000000Z', 'handler_name': '#Mainconcept MP4 Sound Media Handler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 8.21, 'bitrate': 1372, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(Main)', 'video_size': [640, 360], 'video_bitrate': 1200, 'video_fps': 29.97002997002997, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 48000, 'audio_bitrate': 157, 'video_duration': 8.21, 'video_n_frames': 246}\n",
      "/home/jose/CAI2840C/venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i ../Visuals/Main.mp4 -loglevel error -f image2pipe -vf scale=640:360 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../Visuals/Main.mp4\" controls  width=\"600\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First cell: Video Display\n",
    "# This cell demonstrates how to load and display a video using MoviePy\n",
    "\n",
    "# Import VideoFileClip from moviepy.editor - this class handles video file operations\n",
    "# VideoFileClip can read various video formats and provides methods for video manipulation\n",
    "from moviepy import VideoFileClip\n",
    "from IPython.display import display, Video\n",
    "\n",
    "# Specify the path to the input video file\n",
    "# The video is located in the Visuals directory, one level up from current directory\n",
    "input_video = '../Visuals/Main.mp4'\n",
    "\n",
    "# Load the video file into a VideoFileClip object\n",
    "# This creates a clip object that can be manipulated and displayed\n",
    "clip = VideoFileClip(input_video)\n",
    "\n",
    "# The display() function comes from IPython.display module and is specifically designed \n",
    "# for Jupyter notebooks to render rich media content\n",
    "\n",
    "# Video() is a class from IPython.display that creates an HTML5 video player widget\n",
    "# Parameters:\n",
    "#   - input_video: Path to the video file that will be displayed\n",
    "#   - width=600: Sets the width of the video player to 600 pixels\n",
    "#                The height will adjust automatically to maintain aspect ratio\n",
    "\n",
    "# When this line executes:\n",
    "# 1. The Video class loads the video file from the specified path\n",
    "# 2. Creates an HTML5 video player element\n",
    "# 3. The display() function renders the GOAT performing the most famous dunk of all time\n",
    "#    in this video player in the notebook output cell\n",
    "display(Video(input_video, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Part 1: Video I/O Application\n",
    "\n",
    "# Define the paths for input and output videos\n",
    "# We'll read from Main.mp4 and create a grayscale version as output\n",
    "input_video_path = '../Visuals/Main.mp4'\n",
    "output_video_path = '../Visuals/Output_Main_Grayscale.avi'\n",
    "\n",
    "# Initialize video capture object to read from the input video file\n",
    "# cv2.VideoCapture() creates a video capture object that can read frames from a video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Extract important video properties that we'll need for the output video\n",
    "# CAP_PROP_FRAME_WIDTH/HEIGHT: Get the dimensions of each video frame\n",
    "# CAP_PROP_FPS: Get the frames per second of the input video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Set up the video writer object to save the processed frames\n",
    "# fourcc: Four character code that specifies the video codec (mp4v for MP4 format)\n",
    "# isColor=False because we're writing grayscale frames\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height), isColor=False)\n",
    "\n",
    "# Main processing loop: read frames, convert to grayscale, and write to output\n",
    "while cap.isOpened():\n",
    "    # Read a single frame from the video\n",
    "    # ret: Boolean indicating if frame was successfully read\n",
    "    # frame: The actual image data of the frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        # Break the loop if we've reached the end of the video\n",
    "        break\n",
    "\n",
    "    # Convert the frame from BGR color space to grayscale\n",
    "    # OpenCV reads images in BGR (Blue-Green-Red) format by default\n",
    "    # cv2.COLOR_BGR2GRAY converts from BGR color space to single-channel grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Write the processed grayscale frame to the output video\n",
    "    out.write(gray_frame)\n",
    "\n",
    "# Clean up: release video capture and writer objects\n",
    "# This properly closes the files and frees up system resources\n",
    "cap.release()\n",
    "out.release()\n",
    "# Close any OpenCV windows that might have been opened\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def play_video(video_path):\n",
    "    \"\"\"\n",
    "    Function to play a video using OpenCV\n",
    "    Parameters:\n",
    "        video_path (str): Path to the video file\n",
    "    \"\"\"\n",
    "    # Create a video capture object\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    \n",
    "    # Create a window to display the video\n",
    "    window_name = \"Video Player\"\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Break the loop if video is finished\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Display the frame\n",
    "        cv2.imshow(window_name, frame)\n",
    "        \n",
    "        # Wait for 30ms and check if 'q' is pressed to quit\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Play the grayscale video\n",
    "bw_video = '../Visuals/Output_Main_Grayscale.mp4'\n",
    "play_video(bw_video)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': False, 'metadata': {'software': 'Lavf59.27.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [640, 360], 'bitrate': 2323, 'fps': 29.0, 'codec_name': 'mpeg4', 'profile': '(Simple Profile)'}], 'input_number': 0}], 'duration': 8.48, 'bitrate': 2325, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'mpeg4', 'video_profile': '(Simple Profile)', 'video_size': [640, 360], 'video_bitrate': 2323, 'video_fps': 29.0, 'video_duration': 8.48, 'video_n_frames': 245}\n",
      "/home/jose/CAI2840C/venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i ../Visuals/Output_Main_Grayscale.avi -loglevel error -f image2pipe -vf scale=640:360 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../Visuals/Output_Main_Grayscale.avi\" controls  width=\"600\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the path to the grayscale video file\n",
    "# The video is located in the Visuals directory, one level up from current directory\n",
    "bw_video = '../Visuals/Output_Main_Grayscale.avi'\n",
    "\n",
    "# Load the video file into a VideoFileClip object\n",
    "# This creates a clip object that can be manipulated and displayed\n",
    "clip = VideoFileClip(bw_video)\n",
    "\n",
    "# The display() function comes from IPython.display module and is specifically designed \n",
    "# for Jupyter notebooks to render rich media content\n",
    "\n",
    "# Video() is a class from IPython.display that creates an HTML5 video player widget\n",
    "# Parameters:\n",
    "#   - input_video: Path to the video file that will be displayed\n",
    "#   - width=600: Sets the width of the video player to 600 pixels\n",
    "#                The height will adjust automatically to maintain aspect ratio\n",
    "\n",
    "# When this line executes:\n",
    "# 1. The Video class loads the video file from the specified path\n",
    "# 2. Creates an HTML5 video player element\n",
    "# 3. The display() function renders the GOAT performing the most famous dunk of all time\n",
    "#    in this video player in the notebook output cell\n",
    "display(Video(bw_video, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1900, 1900, 1800, 2300, 200, 2000, 2200, 2500, 2600)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.videoio_registry.getBackends())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Motion Detection Application with Foreground Mass and Erosion\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- **Input:** Use a video clip (less than one minute long) that shows some movement.\n",
    "- **Processing:**\n",
    "    - Create a **background model** from the initial frames.\n",
    "    - Compute the **foreground mass** by taking the difference between the current frame and the background model.\n",
    "    - Apply **erosion** to the foreground mask to remove noise (explain how erosion removes small, isolated noise pixels).\n",
    "- **Output:** Annotate the frames (e.g., draw bounding boxes around detected motion areas) and write the output to a new video file.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Background Modeling & Foreground Mass:**\n",
    "2. **Applying Erosion:**\n",
    "3. **Annotation and Video Output:**\n",
    "    - Draw bounding rectangles (or other markers) around the regions of motion detected in the eroded foreground mask.\n",
    "    - Write the annotated frames to a new output video.\n",
    "    \n",
    "Bonus: You can include intermediate visualizations (e.g., display the raw foreground mask vs. the eroded mask) to show the effect of erosion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Contour Detection Application\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    " - **Input:** Use an image that contains distinct shapes (or create one with simple geometric shapes).\n",
    " - **Processing:**\n",
    "    - Convert the image to grayscale and threshold it to create a binary image.\n",
    "    - Use cv2.findContours to detect the contours.\n",
    "    - Use cv2.drawContours to overlay the detected contours on the original image.\n",
    "- **Output:** Display the resulting image with contours drawn.\n",
    "- **Bonus:** Extend your code to process a video stream frame-by-frame, drawing contours on each frame.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Image Preprocessing:**\n",
    "    - Load the image and convert it to grayscale.\n",
    "    - Apply a binary threshold to segment the shapes.\n",
    "2. **Contour Detection:**\n",
    "    - Use cv2.findContours with appropriate retrieval and approximation modes.\n",
    "    - Explain the output: a list of contours and (optionally) a hierarchy that indicates relationships between contours.\n",
    "3. **Drawing the Contours:**\n",
    "    - Use cv2.drawContours to overlay the detected contours onto the image.\n",
    "    - Display the final image.\n",
    "4. **(Bonus) Video Contour Detection:**\n",
    "    - Modify your code from Part 1 to process a video.\n",
    "    - For each frame, detect contours and draw them before writing the frame to an output video file.\n",
    "\n",
    "Hint: Choose a short video clip (under one minute) to ensure quick processing and review."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
